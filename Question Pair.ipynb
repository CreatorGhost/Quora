{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!cp /kaggle/input/quora-question-pairs/train.csv.zip .\n!unzip train.csv.zip\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport nltk\nfrom matplotlib import rcParams\nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.cm as cm\nimport numpy as np\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom subprocess import check_output\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nimport gc\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom nltk.corpus import stopwords\n\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nimport collections\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"train.csv\")\n\nprint(\"Number of data points:\",df.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading Data And Getting Basics Stats","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Here we can see that we have some missing value in question1 (1 Question) and question2 (2 Question)","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"<h3> 3.2.1 Distribution of data points among output classes</h3>","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking if we have imbalance data or not\ndf.groupby(\"is_duplicate\")['id'].count().plot.bar()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking if dataset is balanced or imbalanced","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"unique=len(set(list(df['qid1'])+list(df['qid2'])))\nprint ('Total number of  Unique Questions are: {}\\n'.format(unique))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Percentage of Similar question and Non-Similar Question","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"print('-->> Question pairs are not Similar (is_duplicate = 0):\\n     {}%'.format(100 - round(df['is_duplicate'].mean()*100, 2)))\nprint('-->> Question pairs are Similar (is_duplicate = 1):\\n     {}%'.format(round(df['is_duplicate'].mean()*100, 2)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"qids = pd.Series(list(df['qid1']) + list(df['qid2'])) #Creating a dataframe that contains the question id of both quid1 and quid2\nunique_qs = len(np.unique(qids))  #Numpy array to filter down all qniue elements\nqs_morethan_onetime = np.sum(qids.value_counts() > 1)   # Counts any question that have been repeated more than one time\nprint ('Total number of  Unique Questions are: {}\\n'.format(unique_qs))\n#len(set(list(df['qid1'])+list(df['qid2'])))\n\n\nprint ('Number of unique questions that appear more than one time: {} ({}%)\\n'.format(qs_morethan_onetime,round(qs_morethan_onetime/unique_qs*100,2)))\n\nprint ('Max number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) # Taking the frequency of all question and printing the max of them\n\nq_vals=qids.value_counts()\n\nq_vals=q_vals.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = [\"unique_questions\" , \"Repeated Questions\"]\ny =  [unique_qs , qs_morethan_onetime]\n\nplt.figure(figsize=(8, 6))\nplt.title (\"Plot representing unique and repeated questions  \")\nsns.barplot(x,y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#checking whether there are any repeated pair of questions\n\nduplicateRowsDF = df[df.duplicated(['qid1','qid2'])]   # Collecting all Duplicate data into a dataframe so than we can also see what duplicate value we have\n\nprint (\"Number of duplicate questions : \",duplicateRowsDF.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plotting Questions based on there frequency","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(30, 10))\n\nplt.hist(qids.value_counts(), bins=250)\n\nplt.yscale('log')\n\nplt.title('Log-Histogram of question appearance counts')\n\nplt.xlabel('Number of occurences of question')\n\nplt.ylabel('Number of questions')\n\nprint ('Maximum number of times a single question is repeated: {}\\n'.format(max(qids.value_counts()))) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Checking and Removing Null Value","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df[df.isnull().any(1)] # Checking if any value is null in our dataset\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### As we have very few points with NULL value then its better to remove them","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dropna(inplace=True)  # Droping Null Value\ndf[df.isnull().any(1)] \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Basic Feature Extraction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Q1_Len']=df['question1'].str.len()  # Finding length of Question 1\ndf['Q2_Len']=df['question2'].str.len()  # Finding length of Question 2\n\ndf['Q1_Words']=df['question1'].apply(lambda row : len(row.split()))   # Finding Number of Words in Question 1\ndf['Q2_Words']=df['question2'].apply(lambda row : len(row.split()))   # Finding Number of Words in Question 2\n\n# Fucntion to find the number of common words in Question 1 and Question 2\n\ndef common(row):\n    '''\n    We are converting both Question 1 and Question 2 to set (and also converting them to lower so that every word have same) and finding there intersection so that we can get common words\n    Then we are simply finding the lenth of those common words\n    '''\n    return len((set(row['question1'].lower().split())).intersection(set(row['question2'].lower().split())))\n\n\n# Creating New Column with Number of Common Words\ndf['common_Word'] = df.apply(common, axis=1)\n\n\n# Total Number of Distict words in both question1 and question2\n\ndef total(row):\n    '''\n    Coverting them to lower form then removing extra spaces and them removing the repeted words by converting them to sets \n    Then finding Length of Both the questions and adding them to find total words in both\n    '''\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return (len(w1) + len(w2))\n\n# Getting total number of (unique) words in both question1 and question2\ndf['word_Total'] = df.apply(total, axis=1)\n\n\ndef word_share(row):\n    '''\n    Here we are finding total number of shared word and dividing by total number of words [ (A intersection B)/A+B]\n    '''\n    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))    \n    return 1.0 * len(w1 & w2)/(len(w1) + len(w2))    # Finding the number of common words between question1 and question2 and dividing by total words between both of them\ndf['Shared_Word'] = df.apply(word_share, axis=1)\n\n\n# Saving our dataframe as csv file\ndf.to_csv(\"With_Feature.csv\", index=False)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Analysing our extracted features","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Asking Some Basic Question To Our Extracted Feature","execution_count":null},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"print (\"Minimum length of the questions in question1 : \" , min(df['Q1_Words']))\nprint (\"Minimum length of the questions in question2 : \" , min(df['Q2_Words']))\n\nprint (\"Number of Questions with minimum length [question1] :\", df[df['Q1_Words']== 1].shape[0])\nprint (\"Number of Questions with minimum length [question2] :\", df[df['Q2_Words']== 1].shape[0])\n\nprint (\"Maximum length of the questions in question1 : \" , max(df['Q1_Words']))\nprint (\"Maximum length of the questions in question2 : \" , max(df['Q2_Words']))\n\nprint (\"Number of Questions with minimum length [question1] :\", df[df['Q1_Words']>120].shape[0])\nprint (\"Number of Questions with minimum length [question2] :\", df[df['Q2_Words']> 230].shape[0])\n\nprint (\"Maximum number of Common word : \" , max(df['common_Word']))\nprint (\"Maximum number of Shared Word : \" , max(df['Shared_Word']))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analysing Shared Word","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\n\nplt.subplot(1,2,1)\nsns.violinplot(x = 'is_duplicate', y = 'Shared_Word', data = df[0:])\n\nplt.subplot(1,2,2)\nsns.distplot(df[df['is_duplicate'] == 1.0]['Shared_Word'][0:] , label = \"1\", color = 'red')\nsns.distplot(df[df['is_duplicate'] == 0.0]['Shared_Word'][0:] , label = \"0\" , color = 'green' )\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\n## [2]Text pre-processing\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import SnowballStemmer\nsnow=nltk.stem.SnowballStemmer('english')\n\nstopwords= set(['the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def removeStopWord(word):\n  token=word.split(\" \")   ## coverting string to token (list of word) \\\\ like [\"this\",\"is\",\"token\"]\n  removestop=[snow.stem(x) for x in token if x not in stopwords]   ##removing stopwords and also doing Stemming\n  removed=\" \".join(removestop)  ##joing back the list into sentence\n  return removed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef preprocess(x):\n    x = str(x).lower()\n    x = x.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\")\\\n                           .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" is\")\\\n                           .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \")\\\n                           .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"@\",\"at\")\n    x = re.sub(r\"([0-9]+)000000\", r\"\\1m\", x)\n    x = re.sub(r\"([0-9]+)000\", r\"\\1k\", x)\n    \n    \n    #Remove any special character like [@ , ' ; \"\" ']\n    \n    pattern = re.compile('\\W')\n    \n    if type(x) == type(''):\n        x = re.sub(pattern, ' ', x)\n    \n    #Removing Stopwords And Doing Stemming\n    x=removeStopWord(x)\n    \n               \n    \n    return x","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}